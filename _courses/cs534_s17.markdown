---
layout: page
title: CS 534 - Machine Learning
semester: Spring 2017
---
### Course Overview
Machine learning is an exciting field of computer science that impacts many applications both from a consumer standpoint (e.g., Microsoft Kinect, iPhone's Siri, Netflix recommendations) and the sciences and medicines (e.g., predicting genome-protein interactions, detecting tumors, personalized medicine). In this course, students will learng the fundamental theory and algorithms of machine learning and how to apply machine learning to solve problems.
**Prerequisites**: A course in linear algebra and previous exposure to statistics and probability theory. Homeworks and project will require programming ability in Python, Matlab, R, or C/C++..
### Course Logistics
* **Piazza**: All annoucements, assignment clarifications, and slide corrections will be posted here. Make sure you check it on a regular basis.
* **Office Hours**: (Caveat - Office hours may change from time to time, in which case an announcement will be made on Piazza.)
* Joyce Ho: M 1:30 PM - 3:30 PM, W 9:30 AM - 12:00 PM @ MSC W414
* Rongmei Lin: F 12:00 PM - 2:00 PM @MSC N410
* **Textbooks**
* *Required*: The Elements of Statistical Learning: Data Mining, Inference, and Prediction), by Trevor Hastie, Robert Tibshirani & Jerome Friedman
* Supplemental: Machine Learning: a Probabilistic Perspective, by Kevin Murphy
* Supplemental: Pattern Recognition and Machine Learning, by Christopher Bishop
## Course Schedule
<table class="table">
  <thead>
    <tr><th class="col-md-1">#</th><th class="col-md-1">Date</th><th class="col-md-2">Topic</th><th class="col-md-2">Materials</th><th class="col-md-4">References</th><th class="col-md-3">Assignments</th></tr>
  </thead>
  <tbody>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Introduction and Review</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1/10</td>
      <td class="side-borders">Overview &amp; Course Logistics</td>
      <td class="side-borders">
        <a href="slide/1-intro.pdf">Lecture 1</a><br>
        <a href="code/iPythonIntro.ipynb">iPythonIntro.ipynb</a>
      </td>
      <td class="side-borders">
        <ul>
          <li>Chapter 1 (Hastie et al.)</li>
          <li><a href="http://www.cs.ubc.ca/~murphyk/MLbook/pml-intro-22may12.pdf">Chapter 1 (Murphy)</a></li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>2</td>
      <td>1/12</td>
      <td class="side-borders">Random Variables &amp; Probability Review</td>
      <td class="side-borders">
        <a href="slide/2-prob-review.pdf">Lecture 2</a><br>
        <a href="slide/2-bayesian-methods.pdf">Bayesian Review</a>
      </td>
      <td class="side-borders">
        <ul>
          <li><a href="note/cs229-prob.pdf">Probability theory review</a> from Stanford's machine learning class</li>
          <li>Sam Roweis's <a href="note/probx.pdf">probability review notes</a></li>
          <li>Ingmar Land's <a href="note/prob-cheat-sheet.pdf">probability cheat sheet</a></li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>3</td>
      <td>1/17</td>
      <td class="side-borders">Linear Algebra &amp; Optimization Review</td>
      <td class="side-borders">
        <a href="slide/3-linear-opt-review.pdf">Lecture 3</a><br>
        <a href="code/NumpyBasics.ipynb">NumpyBasics.ipynb</a>
      </td>
      <td class="side-borders">
        <ul>
          <li><a href="note/cs229-linalg.pdf">Linear algebra notes</a> from Stanford's ML class</li>
          <li>Sam Roweis's <a href="note/linear_algebra.pdf">linear algebra review notes</a></li>
          <li>Convex optimization notes <a href="note/cs229-cvxopt.pdf">Part I</a> and  <a href="note/cs229-cvxopt2.pdf">II</a> from Stanford's machine learning class</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Supervised Learning I</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1/19</td>
      <td class="side-borders" rowspan="2">Statistical Decision Theory &amp; Linear Regression</td>
      <td class="side-borders" valign="center" rowspan="2">
        <a href="slide/4-5-linear-reg.pdf">Lectures 4 - 5</a><br>
        <a href="code/PandasBasics.ipynb">PandasBasics.ipynb</a><br>
        <a href="data/Teams.csv">Teams.csv (data file)</a><br>
        <a href="code/LinearRegression.ipynb">LinearRegression.ipynb</a>
      </td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 2.1 - 2.4 (Hastie et al.)</li>
          <li>Chapter 3.1 - 3.4 (Hastie et al.)</li>
          <li>Chapter 17.1 - 17.2 <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/171216.pdf">(Barber)</a></li>
          <li><a href="http://faculty.mccombs.utexas.edu/carlos.carvalho/teaching/Section4.pdf">Prof. Carlos Carvalho MLR Slides</a></li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>5</td>
      <td>1/24</td>
      <td></td>
    </tr>
    <tr>
      <td>6</td>
      <td>1/26</td>
      <td class="side-borders" rowspan="2">Linear Classification</td>
      <td class="side-borders" valign="center" rowspan="2">
        <a href="slide/6-7-linear-class.pdf">Lectures 6-7</a><br>
        <a href="code/LDA.ipynb">LDA.ipynb</a><br>
        <a href="code/LogisticRegression.ipynb">LogisticRegression.ipynb</a><br>
      </td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 4.1 - 4.4 (Hastie et al.)</li>
          <li>Chapter 3.2 - 3.5 (<a href="https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">Mitchell</a>) </li>
        </ul>
      </td>
      <td>
        <ul>
          <li><a href="assignment/HW1.pdf">Homework 1</a></li>
          <li><a href="data/spam.train.dat">Spam Training Data</a></li>
          <li><a href="data/spam.test.dat">Spam Testing Data</a></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>7</td>
      <td>1/31</td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Validation, Model Selection, and Theory</td>
    </tr>
    <tr>
      <td>8</td>
      <td>2/2</td>
      <td class="side-borders">Learning Theory</td>
      <td class="side-borders">
        <a href="slide/8-learning.pdf">Lecture 8</a>
      </td>
      <td class="side-borders" rowspan="4">
        <ul>
          <li>Chapters 7, 8 (Hastie et al.)</li>
          <li><a href="http://cs229.stanford.edu/notes/cs229-notes4.pdf">Learning theory notes</a> from Stanford's ML class</li>
          <li>Chapter 12 (<a href="http://ciml.info/dl/v0_99/ciml-v0_99-ch12.pdf">Daume</a>)</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>9</td>
      <td>2/7</td>
      <td class="side-borders">Validation</td>
      <td class="side-borders">
        <a href="slide/9-validation.pdf">Lecture 9</a><br>
        <a href="code/CrossValidation.ipynb">CrossValidation.ipynb</a>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>10</td>
      <td>2/9</td>
      <td class="side-borders" rowspan="2">Bootstrap &amp; Model Selection</td>
      <td class="side-borders" rowspan="2">
        <a href="slide/10-11-ms-bootstrap.pdf">Lectures 10-11</a>
      </td>
      <td>
        <ul>
          <li><a href="assignment/HW2.pdf">Homework 2</a></li>
        </ul>
      </td>
    </td></tr>
    <tr>
      <td>11</td>
      <td>2/14</td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Supervised Learning II</td>
    </tr>
    <tr>
      <td>12</td>
      <td>2/16</td>
      <td class="side-borders" rowspan="2">Boosting, Trees &amp; Additive Models</td>
      <td class="side-borders" rowspan="2">
        <a href="slide/12-13-boosting-trees.pdf">Lectures 12-13</a><br>
        <a href="code/DecisionTree.ipynb">DecisionTree.ipynb</a>
      </td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapters 9, 10 (Hastie et al.)</li>
          <li>Chapter 3 (<a href="http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf">Mitchell</a>)</li>
          <li>Cynthia Rudin's <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec08.pdf">notes</a> on decision trees</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>13</td>
      <td>2/21</td>
      <td></td>
    </tr>
    <tr>
      <td>14</td>
      <td>2/23</td>
      <td class="side-borders">Ensembles &amp; Random Forests</td>
      <td class="side-borders">
        <a href="slide/14-ensembles.pdf">Lecture 14</a>
      </td>
      <td class="side-borders">
        <ul>
          <li>Chapters 15, 16 (Hastie et al.)</li>
          <li>Breiman's <a href="http://dl.acm.org/citation.cfm?id=570182">paper</a> on random forests</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>15</td>
      <td>2/28</td>
      <td class="side-borders" rowspan="2">Support Vector Machines</td>
      <td class="side-borders" rowspan="2">
        <a href="slide/15-16-svm.pdf">Lectures 15-16</a><br>
        <a href="code/SVM.ipynb">SVM.ipynb</a>
      </td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 12 (Hastie et al.)</li>
          <li>Chapter 15 <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html">(Shalev-Shwartz &amp; Ben-David)</a></li>
          <li><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">SVM notes</a> from Stanford's ML class</li>
          <li><a href="https://davidrosenberg.github.io/ml2015/docs/svm-notes.pdf">SVM notes</a> from NYU's ML class</li>
          <li>Chapter 11 (<a href="http://ciml.info/">Daume</a>)</li>
        </ul>
      </td>
      <td>
        <ul>
          <li><a href="assignment/HW3.pdf">Homework 3</a></li>
          <li><a href="data/allhyper.data">allhyper.data</a></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>16</td>
      <td>3/2</td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Supervised Learning II</td>
    </tr>
    <tr>
      <td>17</td>
      <td>3/14</td>
      <td class="side-borders" rowspan="2">Neural Networks</td>
      <td class="side-borders" rowspan="2">
        <a href="slide/17-18-nn.pdf">Lectures 17-18</a><br>
        <a href="http://playground.tensorflow.org/">Neural Network Playground</a><br>
        <a href="http://scs.ryerson.ca/~aharley/vis/conv/">Handwritten Digit Visualization</a><br>
        <a href="http://scikit-learn.org/stable/_downloads/plot_mnist_filters.ipynb">plot_mnist_filters.ipynb</a> by Scikit-learn
      </td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 11 (Hastie et al.)</li>
          <li><a href="http://cs231n.github.io/">Notes from Convolutional Neural Networks course at Stanford</a></li>
          <li>Chapter 1, 2 <a href="http://neuralnetworksanddeeplearning.com/index.html">(Nielsen)</a></li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>18</td>
      <td>3/16</td>
      <td></td>
    </tr>
    <tr>
      <td>19</td>
      <td>3/21</td>
      <td class="side-borders">K Nearest Neighbors</td>
      <td class="side-borders">
        <a href="slide/19-knn.pdf">Lecture 19</a>
      </td>
      <td class="side-borders">
        <ul>
          <li>Chapter 13 (Hastie et al.)</li>
        </ul>
      </td>
      <td>
        <a href="assignment/HW4.pdf">Homework 4</a>
      </td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Midterm</td>
    </tr>
    <tr>
      <td>20</td>
      <td>3/23</td>
      <td class="side-borders">Midterm</td>
      <td class="side-borders"></td>
      <td class="side-borders"></td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Unsupervised Learning</td>
    </tr>
    <tr>
      <td>21</td>
      <td>3/28</td>
      <td class="side-borders">Dimensionality Reduction</td>
      <td class="side-borders">
        <a href="slide/21-pca.pdf">Lecture 21</a><br>
        <a href="code/DimensionalityReduction.ipynb">DimensionalityReduction.ipynb</a>
      </td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 14 (Hastie et al.)</li>
          <li>Chapter 23 - 23.1 <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html">(Shalev-Shwartz &amp; Ben-David)</a></li>
          <li><a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf">PCA notes</a> from Stanford's ML class</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>22</td>
      <td>3/30</td>
      <td class="side-borders">Clustering &amp; Mixture Models</td>
      <td class="side-borders">
        <a href="slide/22-cluster.pdf">Lecture 22</a>
      </td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Other Topics</td>
    </tr>
    <tr>
      <td>23</td>
      <td>4/4</td>
      <td class="side-borders">Topic Models</td>
      <td class="side-borders">
        <a href="slide/23-topics.pdf">Lecture 23</a>
      </td>
      <td class="side-borders">
        <ul>
          <li>Blei's <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">review article</a></li>
        </ul>
      </td>
      <td>
        <ul>
          <li><a href="assignment/HW5.pdf">Homework 5</a></li>
          <li><a href="data/Colleges.txt">Colleges.txt</a></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>24</td>
      <td>4/6</td>
      <td class="side-borders">Hidden Markov Models</td>
      <td class="side-borders">
        <a href="slide/24-hmm.pdf">Lecture 24</a>
      </td>
      <td class="side-borders">
        <ul>
          <li>Murphy's <a href="http://www.cs.ubc.ca/~murphyk/Papers/intro_gm.pdf">intro to graphical models</a></li>
          <li><a href="http://digital.cs.usu.edu/~cyan/CS7960/hmm-tutorial.pdf">HMM tutorial</a> by Blunsom</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>25</td>
      <td>4/11</td>
      <td class="side-borders">Deep Learning</td>
      <td class="side-borders">
        <a href="slide/25-deep-learning.pdf">Lecture 25</a>
      </td>
      <td class="side-borders"></td>
      <td></td>
    </tr>
    <tr>
      <td>26</td>
      <td>4/13</td>
      <td class="side-borders">Recommendation Systems</td>
      <td class="side-borders">
        <a href="slide/26-recommendation.pdf">Lecture 26</a>
      </td>
      <td class="side-borders">
        <ul>
          <li>Chapter 9 (<a href="http://infolab.stanford.edu/~ullman/mmds/ch9.pdf">Leskovec et al.</a>)</li>
          <li><a href="http://ieeexplore.ieee.org/document/5197422">Paper</a> by Koren et al.</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Project Presentations</td>
    </tr>
    <tr>
      <td>27</td>
      <td>4/18</td>
      <td class="side-borders" rowspan="2">Presentations</td>
      <td class="side-borders" rowspan="2"></td>
      <td class="side-borders" rowspan="2"></td>
      <td></td>
    </tr>
    <tr>
      <td>28</td>
      <td>4/20</td>
      <td></td>
    </tr>
  </tbody>
</table>

### Course Grading

|Component   | Weight |
|:---------- |:------ |
|Homeworks   | 40%    |
|Midterm   | 15%    |
|Project   | 40%    |
|Participation   |  5% |

### Assignment and Exam Policy
**Assignments**
* Assignments are due electronically at 11:59 PM on Canvas.
* Each student receives six late days that can be used on assignments throughout the semester. These late days extend the deadline for 24 hours, and can be distributed amongst the 4-5 assignments, but no more than 3 late days may be used on any assignment.
* After the six late days are used, the assignment will be penalized 10% off per day up to 3 late days.
* Late days apply to the entire assignment, so handing in one problem late counts as a late day towards the whole assignment.
* After 3 late days on any given assignment you will receive no credit for the asignment.
**Exam**
* All exams must be taken promptly at the required time.
* Requests for rescheduling the midterm exam will only be considered if the request is made at least a week prior to the exam date.
The above policies will be waived only in an "emergency" situation with appropriate documentation.
## Honor Code
All class work is governed by the College Honor Code and Departmental Policy.
It is acceptable and encouraged to discuss homeworks with other students.
However, this should be noted on your submitted homework and _all code and writeup_ must be written by yourself.
Any code and writeup that is found to be similar is grounds for an honor code investigation by the Director of Gradute Studies, Laney Graduate School, and the honor council.