---
layout: page
title: CS 534 - Machine Learning
semester: Fall 2017
---
## Course Overview
Machine learning impacts many applications including the sciences (e.g., predicting genome-protein interactions, detecting tumors, personalized medicine) and consumer products (e.g., Amazon's Alexa, Microsoft Kinect, Neflix).
In this course, students will learn the fundamental theory and algorithms of machine learning.
Students will also obtain practical experience applying standard machine learning methods to solve a variety of problems.
*Prerequisites*:
* Undergraduate-level linear algebra
* Undergraduate-level probability
* Exposure to statistics
* Programming ability in Python, Matlab, Julia, R, or C++
## Course Logistics
* Piazza: All annoucements, assignment clarifications, and slide corrections will be posted here. Make sure you check it on a regular basis.
* Office Hours:
* Joyce Ho: M 1:30 PM - 3:30 PM, W 9:30 AM - 12:00 PM @ MSC W414
* Textbook
* Required: The Elements of Statistical Learning: Data Mining, Inference, and Prediction), by Trevor Hastie, Robert Tibshirani & Jerome Friedman
* Supplemental: Machine Learning: a Probabilistic Perspective, by Kevin Murphy
* Supplemental: Pattern Recognition and Machine Learning, by Christopher Bishop
## (Tentative) Course Schedule
The reading material listed below is optional and the lecture plan may deviate over the course of the semester.
<table class="table">
  <thead>
    <tr><th class="col-md-1">#</th><th class="col-md-1">Date</th><th class="col-md-2">Topic</th><th class="col-md-4">References</th><th class="col-md-3">Assignments</th></tr>
  </thead>
  <tbody>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Introduction</td>
    </tr>
    <tr>
      <td>1</td>
      <td>8/23</td>
      <td class="side-borders">Overview &amp; Course Logistics</td>
      <td class="side-borders">
        <ul>
          <li>Chapter 1 (Hastie et al.)</li>
          <li><a href="http://www.cs.ubc.ca/~murphyk/MLbook/pml-intro-22may12.pdf">Chapter 1 (Murphy)</a></li>
        </ul>
      </td>
      <td>
        <ul>
          <li>Homework 0 (due 8/27)</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>2</td>
      <td>8/29</td>
      <td class="side-borders">Crash Course in Optimization and Statistics</td>
      <td class="side-borders">
        <ul>
          <li>Convex optimization notes <a href="../cs534_s17/note/cs229-cvxopt.pdf">Part I</a> and  <a href="../cs534_s17/note/cs229-cvxopt2.pdf">II</a> from Stanford's machine learning class</li>
          <li>Rosenberg's <a href="https://davidrosenberg.github.io/mlcourse/Notes/convex-optimization.pdf">abridged notes</a></li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Supervised Learning I</td>
    </tr>
    <tr>
      <td>3</td>
      <td>8/31</td>
      <td class="side-borders" rowspan="2">Linear Regression</td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 3.1 - 3.4 (Hastie et al.)</li>
          <li>Chapter 17.1 - 17.2 <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/171216.pdf">(Barber)</a></li>
          <li><a href="http://faculty.mccombs.utexas.edu/carlos.carvalho/teaching/Section4.pdf">Prof. Carlos Carvalho MLR Slides</a></li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>4</td>
      <td>9/5</td>
      <td></td>
    </tr>
    <tr>
      <td>5</td>
      <td>9/7</td>
      <td class="side-borders" rowspan="2">Naive Bayes &amp; Linear Classification</td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 2.1 - 2.4 (Hastie et al.)</li>
          <li>Chapter 4.1 - 4.4 (Hastie et al.)</li>
          <li>Chapter 3.2 - 3.5 (<a href="https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">Mitchell</a>) </li>
        </ul>
      </td>
      <td>
        <ul>
          <li>Homework 1 (due 9/22)</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>6</td>
      <td>9/12</td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Model Assessment &amp; Selection</td>
    </tr>
    <tr>
      <td>7</td>
      <td>9/14</td>
      <td class="side-borders">Bias &amp; Variance Tradeoff</td>
      <td class="side-borders" rowspan="4">
        <ul>
          <li>Chapters 7, 8 (Hastie et al.)</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>8</td>
      <td>9/19</td>
      <td class="side-borders">Model Assessment</td>
      <td></td>
    </tr>
    <tr>
      <td>9</td>
      <td>9/21</td>
      <td class="side-borders" rowspan="2">Bootstrap &amp; Model Selection</td>
      <td>
        <ul>
          <li>Homework 2 (due 10/6)</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>10</td>
      <td>9/26</td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Supervised Learning II</td>
    </tr>
    <tr>
      <td>11</td>
      <td>9/28</td>
      <td class="side-borders" rowspan="2">Boosting, Trees &amp; Additive Models</td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapters 9, 10 (Hastie et al.)</li>
          <li>Chapter 3 (<a href="http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf">Mitchell</a>)</li>
          <li>Cynthia Rudin's <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec08.pdf">notes</a> on decision trees</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>12</td>
      <td>10/3</td>
      <td></td>
    </tr>
    <tr>
      <td>13</td>
      <td>10/5</td>
      <td class="side-borders">Ensembles &amp; Random Forests</td>
      <td class="side-borders">
        <ul>
          <li>Chapters 15, 16 (Hastie et al.)</li>
          <li>Breiman's <a href="http://dl.acm.org/citation.cfm?id=570182">paper</a> on random forests</li>
        </ul>
      </td>
      <td>
        <ul>
          <li>Homework 3 (due 10/20)</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>14</td>
      <td>10/12</td>
      <td class="side-borders" rowspan="2">Support Vector Machines</td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 12 (Hastie et al.)</li>
          <li>Chapter 15 <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html">(Shalev-Shwartz &amp; Ben-David)</a></li>
          <li><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">SVM notes</a> from Stanford's ML class</li>
          <li><a href="https://davidrosenberg.github.io/ml2015/docs/svm-notes.pdf">SVM notes</a> from NYU's ML class</li>
          <li>Chapter 11 (<a href="http://ciml.info/">Daume</a>)</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>15</td>
      <td>10/17</td>
      <td></td>
    </tr>
    <tr>
      <td>16</td>
      <td>10/19</td>
      <td class="side-borders" rowspan="2">Neural Networks</td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 11 (Hastie et al.)</li>
          <li>Chapter 1-3 <a href="http://neuralnetworksanddeeplearning.com/index.html">(Nielsen)</a></li>
        </ul>
      </td>
      <td>
        <ul>
          <li>Homework 4 (due 11/3)</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>17</td>
      <td>10/24</td>
      <td></td>
    </tr>
    <tr>
      <td>18</td>
      <td>10/26</td>
      <td class="side-borders">K Nearest Neighbors</td>
      <td class="side-borders">
        <ul>
          <li>Chapter 13 (Hastie et al.)</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Unsupervised Learning</td>
    </tr>
    <tr>
      <td>19</td>
      <td>10/31</td>
      <td class="side-borders">Dimensionality Reduction</td>
      <td class="side-borders" rowspan="2">
        <ul>
          <li>Chapter 14 (Hastie et al.)</li>
          <li>Chapter 23 - 23.1 <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html">(Shalev-Shwartz &amp; Ben-David)</a></li>
          <li><a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf">PCA notes</a> from Stanford's ML class</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>20</td>
      <td>11/2</td>
      <td class="side-borders">Clustering &amp; Mixture Models</td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Midterm</td>
    </tr>
    <tr>
      <td>21</td>
      <td>11/7</td>
      <td class="side-borders">Midterm</td>
      <td class="side-borders"></td>
      <td></td>
    </tr>
    <tr>
      <td>22</td>
      <td>11/9</td>
      <td class="side-borders">Project Madness &amp; TBD</td>
      <td class="side-borders"></td>
      <td>
        <ul>
          <li>Homework 5 (due 12/1)</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Other Topics</td>
    </tr>
    <tr>
      <td>23</td>
      <td>11/14</td>
      <td class="side-borders">Hidden Markov Models</td>
      <td class="side-borders">
        <ul>
          <li>Murphy's <a href="http://www.cs.ubc.ca/~murphyk/Papers/intro_gm.pdf">intro to graphical models</a></li>
          <li><a href="http://digital.cs.usu.edu/~cyan/CS7960/hmm-tutorial.pdf">HMM tutorial</a> by Blunsom</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>24</td>
      <td>11/16</td>
      <td class="side-borders" rowspan="2">Deep Learning</td>
      <td class="side-borders" rowspan="2"></td>
      <td></td>
    </tr>
    <tr>
      <td>25</td>
      <td>11/21</td>
      <td></td>
    </tr>
    <tr>
      <td>26</td>
      <td>11/28</td>
      <td class="side-borders">Topic Models</td>
      <td class="side-borders">
        <ul>
          <li>Blei's <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">review article</a></li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td>27</td>
      <td>11/30</td>
      <td class="side-borders">Recommendation Systems</td>
      <td class="side-borders">
        <ul>
          <li>Chapter 9 (<a href="http://infolab.stanford.edu/~ullman/mmds/ch9.pdf">Leskovec et al.</a>)</li>
          <li><a href="http://ieeexplore.ieee.org/document/5197422">Paper</a> by Koren et al.</li>
        </ul>
      </td>
      <td></td>
    </tr>
    <tr>
      <td align="center" colspan="6" class="plan-sec-header">Project Presentations</td>
    </tr>
    <tr>
      <td>28</td>
      <td>12/5</td>
      <td class="side-borders" rowspan="2">Presentations</td>
      <td class="side-borders" rowspan="2"></td>
    </tr>
    <tr>
      <td>29</td>
      <td>12/7</td>
    </tr>
  </tbody>
</table>

## Course Grading

Component   | Weight |
--------- | ------ |
Homeworks   | 35.0%    |
Midterm   | 17.5%    |
Project   | 40.0%    |
Participation   |  7.5% |

## Project

You are encouraged to work in groups of 2-3 for the final project.
The goal is to either develop a novel algorithm (novelty bonus points will be given depending on the level of difficulty) or try various ML existing algorithms on the dataset.
The project is a critical part of the course and a significant factor in determining your grade.
Teams are required to hand in a project proposal, a final project report and prepare two presentations on their work.
By default, all team members will receive the same score for their project.
If a team feels that this is unfair perhaps due to *HIGHLY* imbalanced contributions,
then every team member needs to provide feedback on the contribution of each of the other team members via email before submission of the final report.
After that I will have a meeting with the entire group to mediate.
More details on projects are posted on Piazza under the projects folder.

Component   | Due Date | Weight |
--------- | -------- | -------|
Proposal    | 10/25    | 15%    |
Madness   | 11/8     | 10%    |
Presentation |12/4-12/11 |  25% |
Report    |12/12       | 50%    |

## Assignment and Exam Policy

* Assignments
* Assignments (homeworks 1-5, final projects) are due electronically on Canvas at 11:59 PM.
* Each student receives 6 late days that can be used across the 5 homeworks throughout the semester. These late days extend the deadline for 24 hours.
* A maximum of 3 late days can be used on a given homework.
* Late days apply to the entire homework, so handing in one problem late counts as a late day towards the whole homework.
* **No credit** will be given if you submit the homework late and have no remaining late days.
* Exam
* The midterm (open-book, open-notes, no electronic devices) must be taken at the required time.
* Requests for rescheduling the midterm exam will only be considered if the request is made at least a week prior to the exam date.
## Honor Code
All class work is governed by the College Honor Code and Departmental Policy.
It is acceptable and encouraged to discuss homeworks with other students.
However, this should be noted on your submitted homework and _all code and writeup_ must be written by yourself.
Any code and writeup that is found to be similar is grounds for an honor code investigation by the Director of Gradute Studies, Laney Graduate School, and the honor council.